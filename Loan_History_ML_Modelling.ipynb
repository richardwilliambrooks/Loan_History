{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Loan Eligibility Classification Model\n",
    "#\n",
    "# The script creates a logistic regression model to calculate whether an applicant is eligible for a loan.\n",
    "# This based on a number of catagorical and continous data fields\n",
    "# Some of the fields are calculated new independant variables that may give better linear dependancies (potentially) \n",
    "# more inisightful (such as Loan to Income)\n",
    "\n",
    "# The training data set was analysed (in the Loan_History_ML_Eval module) to understand the data\n",
    "# The script uses shared functions (defined in fn_Aux_Loan_History) to process the data\n",
    "\n",
    "# The training and test data are loaded\n",
    "\n",
    "# The user then defines\n",
    "# .... the subset of parameters to use, \n",
    "# .... the columns where null values are filled (catagorical - most poular, numeical mean)\n",
    "# .... the cost function to evalaute the performance of the model (of two in library set)\n",
    "\n",
    "# The model is parameterised based on a set of training data\n",
    "# The training is done by a series of runs (bootstrapping)\n",
    "# ... splitting the data into a training and verification set\n",
    "# ... for a number of variations of regulaisation parameter C (a high value fits the data less tightly)\n",
    "# ... a score is given based on the fraction of values defined as accepted which are true and false (TP and FN)\n",
    "# ... as this will determine the cost (weighted by the user) of a false acceptance and the benefit of a true acceptance\n",
    "# ... the training identification output gives the results for every model run\n",
    "\n",
    "# The training output is analysed (graphically) to asses the regulaisation parameter C that gives the best model (cost)\n",
    "\n",
    "# The model is created using by running all the training data with the latest tuning data\n",
    "# The tuning coefficents are presented to give an insight into how the parameter selection can be adapted\n",
    "\n",
    "# The model is reran against the test data provided to indicate which new applicants should be recomended for approval\n",
    "# The results are saved (if option is selected) as a pickle for storage and further analysis\n",
    "\n",
    "# The interntion is that the script is ran iteratively with the parameter combinations selected by the user\n",
    "# This could be modified (in future) to allow combinations of parameters to be ran in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "# Rev     By                Description\n",
    "# 1.1    Richard Brooks     Tidy Up\n",
    "# 1.0    Richard Brooks     Initial Release\n",
    "\n",
    "sScript = 'Loan History Modelling'\n",
    "sVersion = 'v1.1'\n",
    "sAuthor = 'Richard W Brooks'\n",
    "\n",
    "print ('Running : ' + sScript + ' : ' + sVersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing python libraries and running file with bespoke functions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import datetime as dt\n",
    "import random as rd\n",
    "import scipy.stats as st\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# 0. Loading and defining functions\n",
    "# -------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run fn_Aux_Loan_History.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions options for the cost function are defined as dictionaries so that the corresponding headings can be extracted\n",
    "# These input actual and predicted values, (and if required parameters)\n",
    "# They output a score - conistent with teh header\n",
    "\n",
    "# Weighted Average Cost\n",
    "# Gives a weighted average score based on a set of weights\n",
    "# Inputs : weighting given to ['TN', 'FP', 'FN', 'TP'] \n",
    "# Outputs : prediction ['TN', 'FP', 'FN', 'TP'], 'T' (number predicted as true), 'SCORE' (weighted score, based on input)\n",
    "# ... caluates the confusion metric coverts to fraction, then True and Weighted Score based on array input\n",
    "\n",
    "ls_Weight_Model_Score_Fields = ['TN', 'FP', 'FN', 'TP', 'T', 'SCORE']\n",
    "\n",
    "def fn_ls_Weight_Model_Score(y_act, y_pred, *args):\n",
    "    \n",
    "        ls_weight = args[0]\n",
    "        valid_confusion = confusion_matrix(y_act, y_pred, labels=[0,1])\n",
    "        ls_vc = valid_confusion.flatten().tolist()\n",
    "        ls_vc = [i/sum(ls_vc) for i in  ls_vc]\n",
    "        \n",
    "        t = sum([x * y for x, y in zip(ls_vc, [0,0,1,1])])\n",
    "        ws = sum([x * y for x, y in zip(ls_vc,ls_weight)])\n",
    "        \n",
    "        ls_sc = ls_vc + [t, ws]\n",
    "        \n",
    "        return ls_sc\n",
    "    \n",
    "dict_Weight_Model_Score = {'ls_Fields':ls_Weight_Model_Score_Fields, 'fn' : fn_ls_Weight_Model_Score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Under Curve\n",
    "# Gives the area under the False Possotive / True Possotive Curve - the closer to 1 the better the model\n",
    "# Inputs : no additional\n",
    "# Outputs : prediction ['Precsion : TP/(TP+FP)', 'Recall : TP/(TP+FN)', Area Under Curve)\n",
    "\n",
    "ls_AUC_Score_Fields = ['PREC', 'RECALL', 'SCORE']\n",
    "\n",
    "def fn_ls_AUC_Score(y_act, y_pred, *args):\n",
    "\n",
    "        p = precision_score(y_act, y_pred)\n",
    "        r = recall_score(y_act, y_pred)\n",
    "        auc = roc_auc_score(y_act, y_pred)   \n",
    "        ls_sc = [p, r, auc]\n",
    "        \n",
    "        return ls_sc\n",
    "    \n",
    "dict_AUC_Model_Score = {'ls_Fields':ls_AUC_Score_Fields, 'fn' : fn_ls_AUC_Score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# 1. Loading the raw data and pre-processing\n",
    "# -------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data sets provided\n",
    "df_test_raw = pd.read_csv('test_Y3wMUE5_7gLdaTN.csv')\n",
    "df_train_raw = pd.read_csv('train_u6lujuX_CVtuZ9i.csv')\n",
    "\n",
    "df_test = df_test_raw.copy()\n",
    "df_train = df_train_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre processing the data\n",
    "\n",
    "# Translate the Output in the training set to a 0/1 value for training\n",
    "df_train['Loan_Status'] = (df_train['Loan_Status']=='Y').astype(int)\n",
    "\n",
    "# Change the format of fields and create calculated parameters for inputs (defined in auxillary functions)\n",
    "df_PreProcess_Data(df_train)\n",
    "df_PreProcess_Data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fraction of fields in training and test data that have NaN values\n",
    "# Show this to inform the user for filling NA values\n",
    "\n",
    "# Columns with null values\n",
    "iRow = df_test.shape[0]\n",
    "df_test_NA = df_test.isna().sum().sort_values()\n",
    "df_test_NA = df_test_NA[df_test_NA>0]/iRow\n",
    "\n",
    "# Columns with null values\n",
    "iRow = df_train.shape[0]\n",
    "df_train_NA = df_train.isna().sum().sort_values()\n",
    "df_train_NA = df_train_NA[df_train_NA>0]/iRow\n",
    "\n",
    "df_train_test_NA = df_train_NA.to_frame().rename(columns = {0:'train_NA'}).join(df_test_NA.to_frame().rename(columns = {0:'test_NA'}))\n",
    "\n",
    "df_train_test_NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paramters with defined types (found by analysis / inspection in the model evaluation)\n",
    "\n",
    "# The output to be predicted\n",
    "ls_Col_Out = ['Loan_Status']\n",
    "\n",
    "# The catagorical variables in the data set\n",
    "ls_Col_Cat = ['Dependents','Education', 'Property_Area','Gender','Married','Self_Employed','Credit_History']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the user what columns are in the data set - have these at hand to copy/psate for selection\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# 2. User Inputs\n",
    "# -------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The user defines how the data is to be modelled\n",
    "# ... these inputs can be adjusted to refine the model based on results\n",
    "\n",
    "# The user then defines\n",
    "# .... the subset of parameters to use, \n",
    "\n",
    "# The independant cause varaibles to be chosen for consideration in the model\n",
    "ls_Col_Indie = ['Education', 'Property_Area','Married','Credit_History','c_JointIncome','c_Loan_per_AppIncome']\n",
    "\n",
    "# The varaibles where an assumption is made to fill NA values  (catagorical - most poular, numeical mean)\n",
    "# ... if the varaiable is not in this list then the row containing NA value is deleted\n",
    "ls_Col_Fill_NA = ['Married', 'Gender', 'Loan_Amount_Term', 'Self_Employed', 'Credit_History']\n",
    "\n",
    "# .... the cost function to evalaute the performance of the model (of two in library set)\n",
    "# Comment out the ones not used\n",
    "\n",
    "# OPTION A: Weighted Average : True Possotive gives income, False Possotive gives loss\n",
    "# Change here to select the cost function and output columns from list\n",
    "dict_Model_Cost = dict_Weight_Model_Score\n",
    "arg_Model_Cost = [0, -1, 0, 1] # Cost Weight -  ['TN', 'FP', 'FN', 'TP']\n",
    "\n",
    "# OPTION B: Area Under Curve : Fraction of distingusihing good / bad loan\n",
    "#dict_Model_Cost = dict_AUC_Model_Score\n",
    "#arg_Model_Cost = np.NaN\n",
    "\n",
    "# .... the setting for parameter tuning\n",
    "\n",
    "# Vales of Regularisation Coeffient 'C', large value simpler model \n",
    "ls_Reg_COpts = [0.1, 0.5, 1, 2.5, 5, 7.5, 10, 25, 50] \n",
    "\n",
    " # Fraction of data used for training and validation\n",
    "d_Valid_Data_Perc = 0.2\n",
    "\n",
    "# Number of runs per evaluation (of co-efficent)\n",
    "i_BootstrapRuns = 750 \n",
    "\n",
    "# ... Whether want to save the main results (as a Pickle) and the associated filename\n",
    "bSaveOutputPickle = True\n",
    "sOutputFileBase = 'Loan_History_LR_Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# 3. Model Training (Bootstrap)\n",
    "# -------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Training Data for Model Evaluation\n",
    "\n",
    "# Select the columns in the training data to use\n",
    "# Process the NA values - if they are in the list select ... delete the rows for values with remaining NA values\n",
    "# Normalise the data - so all is between 0 and 1, catagorical data regers to the fields with 1 if matches, 0 of not\n",
    "\n",
    "ls_Col = list(set(ls_Col_Out + ls_Col_Indie))\n",
    "df_train = df_FillNA_Data(df_train, ls_Col, ls_Col_Fill_NA, ls_Col_Cat)\n",
    "\n",
    "df_train_p = df_Prep_Data(df_train,ls_Col_Cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a set of model evaluations\n",
    "# Conduct a set of runs where split the training data into two sets (identification and validation)\n",
    "# ....For the options of regularisation parameters (C):\n",
    "# ... .... Fit the model based on the the training set, evlaute based on the validation set using the score\n",
    "\n",
    "ls_Score = []\n",
    "iSet = 1 # the script is just evaluating one set of parameter selection, this may be more if the script is adapted in future\n",
    "\n",
    "for iRun in range(i_BootstrapRuns):\n",
    "    \n",
    "    # Splitting the data\n",
    "    df_ident_p, df_valid_p = train_test_split(df_train_p, test_size=d_Valid_Data_Perc)\n",
    "\n",
    "    #Select the output data\n",
    "    y_train = df_ident_p[ls_Col_Out].copy().values.ravel()\n",
    "    y_valid = df_valid_p[ls_Col_Out].copy().values.ravel()\n",
    "\n",
    "    #The remaining columns are inputs\n",
    "    ls_Col_x_p = list(set(df_train_p.columns) - set(ls_Col_Out))\n",
    "    x_train = df_ident_p[ls_Col_x_p]\n",
    "    x_valid = df_valid_p[ls_Col_x_p]\n",
    "    \n",
    "    \n",
    "    # For each regularisation parameter\n",
    "    for CVal in ls_Reg_COpts:\n",
    "        \n",
    "        # Run the model on the training set\n",
    "        clf = LogisticRegression(C = CVal, max_iter=1000,solver='lbfgs').fit(x_train, y_train)\n",
    "        \n",
    "        # Evaluate the model on the test set and give it a score based on the function\n",
    "        y_valid_pred = clf.predict(x_valid)\n",
    "        ls_sc = dict_Model_Cost.get('fn')(y_valid, y_valid_pred, arg_Model_Cost)\n",
    "\n",
    "        # Keep appending the results for each run, using parameters to define the run and the score\n",
    "        ls = [iSet, iRun, CVal] + ls_sc\n",
    "        ls_Score.append(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the scoring based on a dummy classfier as a comparison\n",
    "# ... In the absence of any data you could just assign at random based on the fraction previously accepted\n",
    "# ... This therefore just compares two splits of the outputs based on the size of the validation data\n",
    "\n",
    "ls_Dummy_Score = []\n",
    "\n",
    "y_train = df_train['Loan_Status'].tolist()\n",
    "iSample = int(len(y_train )*d_Valid_Data_Perc)\n",
    "ls_Dummy_Score = []\n",
    "\n",
    "# Take two data splits and run teh model function\n",
    "# In the list define the parameter set and C value to 0\n",
    "for iRun in range(i_BootstrapRuns):\n",
    "    y_valid = rd.sample(y_train,iSample)\n",
    "    y_dummy_pred = rd.sample(y_train,iSample)\n",
    "    ls_sc = dict_Model_Cost.get('fn')(y_valid, y_dummy_pred, arg_Model_Cost)\n",
    "    \n",
    "    ls = [0, iRun, 0] + ls_sc\n",
    "    ls_Dummy_Score.append(ls)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the run score to data list to interogate\n",
    "# This pulls the varaible list associated with the cost function (that is why store in dictionary)\n",
    "df_Sc = pd.DataFrame(ls_Score + ls_Dummy_Score, columns=['Pm Set', 'Run', 'C'] + dict_Model_Cost.get('ls_Fields'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# 4. Model Training Evaluation\n",
    "# -------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the mean results of the runs to assess average sucess\n",
    "# Compare on the graph the fraction of true values in the training data (as a check)\n",
    "\n",
    "df_SC_mean = df_Sc[df_Sc['C']>0].groupby(['C','Pm Set']).mean()\n",
    "df_SC_mean['T Train'] = (df_train['Loan_Status'].value_counts()/len(df_train))[1]\n",
    "\n",
    "df_SC_mean.plot(kind='line', y = dict_Model_Cost.get('ls_Fields') + ['T Train'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution in the results for the individal runs (as an indication of outliers)\n",
    "# Include the dummy classification C = 0 to compare against\n",
    "# This could be copied and pasted if wnat to look at more score varaibles\n",
    "\n",
    "df_Sc_L = pd.melt(df_Sc ,id_vars=['C','Pm Set','Run'], var_name='Field', value_name='Val')\n",
    "sns.catplot(x = \"Val\", y = \"Field\", hue = 'C', kind = \"violin\", data = df_Sc_L[df_Sc_L['Field'] == 'SCORE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model parameter\n",
    "# The score is the maximum value at 95% lower confidence limit\n",
    "\n",
    "# This could / should be done by eyeballing the results to where the score falls off\n",
    "# to better inform the choice of parameter and how it comapred with other runs\n",
    "\n",
    "ls_Score_CI = []\n",
    "d_CI = 0.95\n",
    "\n",
    "for CVal in df_Sc['C'].unique():\n",
    "    ls = df_Sc['SCORE'][df_Sc['C'] == CVal]\n",
    "    d_Sc_CI = st.t.interval(alpha=d_CI, df=len(ls)-1, loc=np.mean(ls), scale=st.sem(ls))\n",
    "    ls_Score_CI.append([CVal, d_Sc_CI[0], d_Sc_CI[1] - d_Sc_CI[0],] )\n",
    "    \n",
    "df_Score_CI = pd.DataFrame(ls_Score_CI, columns=['C','SCORE','BAND' ])\n",
    "df_Score_CI = df_Score_CI[df_Score_CI['C']>0]\n",
    "df_Score_CI.set_index('C', inplace = True)\n",
    "\n",
    "CSel = df_Score_CI.idxmax(axis=0)[0]\n",
    "print ('Regularisation Parameter Selected C : ' + str(CSel))\n",
    "\n",
    "df_Score_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# 5. Run the model on the training and test data\n",
    "# -------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the selected model on the full training data set (so teh model is tuned against the maximum available data)\n",
    "\n",
    "y_train = df_train_p[ls_Col_Out].copy().values.ravel()\n",
    "\n",
    "ls_Col_x_p = list(set(df_train_p.columns) - set(ls_Col_Out))\n",
    "x_train = df_train_p[ls_Col_x_p]\n",
    "\n",
    "clf_sel = LogisticRegression(C = CSel, max_iter=1000,solver='lbfgs').fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the parameters and display these - to give an indication of how important the parmaeters are\n",
    "# The ones closest to zero may be removed in the next evlauation\n",
    "\n",
    "df_Model_Coeff = pd.DataFrame({'Parm' : ls_Col_x_p, 'Coff' : clf_sel.coef_.tolist()[0]}).sort_values('Parm')\n",
    "df_Model_Coeff.set_index('Parm', inplace = True)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(df_Model_Coeff,cmap='coolwarm',annot=True, fmt='.2f', center= 0, linewidths=2, linecolor='black')\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the results of the model on the Training data\n",
    "\n",
    "y_train_pred = clf_sel.predict(df_train_p[ls_Col_x_p])\n",
    "ls_sc_pred = dict_Model_Cost.get('fn')(df_train_p[ls_Col_Out], y_train_pred, arg_Model_Cost)\n",
    "\n",
    "df_train_sc = pd.DataFrame({'Field':dict_Model_Cost.get('ls_Fields'), 'Val': ls_sc_pred})\n",
    "df_train_sc.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# 6. Generate the prediction for the test data\n",
    "# -------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the test data for running the model\n",
    "\n",
    "# Handle NaN and normalise values as per the training data\n",
    "df_test = df_FillNA_Data(df_test, ls_Col_Indie, ls_Col_Fill_NA, ls_Col_Cat)\n",
    "df_test_p = df_Prep_Data(df_test,ls_Col_Cat)\n",
    "\n",
    "# Check and handle any columns that are missing - (as there may be some catagory differences)\n",
    "ls_Add_Col = list(set(df_test_p.columns) - set(df_Model_Coeff.index).intersection(ls_Col_x_p))\n",
    "\n",
    "if len(ls_Add_Col) == 0:\n",
    "    print ('INFO : All columns in test set exist in training set')\n",
    "else:\n",
    "    print ('WARNING : Adding Additional Columns as NAN' + ', '.join(map(str, ls_Add_Col)))\n",
    "    for sCol in ls_Add_Col:\n",
    "        df_test_p[sCol] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "y_train_test = clf_sel.predict(df_test_p)   \n",
    "\n",
    "# Add the output to the test data\n",
    "df_test_results = df_test.copy()\n",
    "df_test_results[ls_Col_Out[0]] = y_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide an indication of the loans that would be approved by applying the model\n",
    "\n",
    "# ... looks the training data actuals and model predicton\n",
    "# ... how this compares to the test data - whether this can be explained by higher values of fields and coefficients\n",
    "\n",
    "print('Training Data : {0:.3f}'.format(y_train.sum()/len(y_train)))\n",
    "print('Training Data Prediction : {0:.3f}'.format(y_train_pred.sum()/len(y_train_pred)))\n",
    "print('Test Data Prediction : {0:.3f}'.format(y_train_test.sum()/len(y_train_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# 7. Colate and Save Parameters and Output\n",
    "# -------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the key parameter values\n",
    "dict_Output = {'clf_sel':clf_sel, 'df_test_results':df_test_results, 'df_train_p':df_train_p,\n",
    "              'df_Sc' : df_Sc, 'sVersion' : sVersion,\n",
    "              'ls_Col_Indie' :  ls_Col_Indie, 'ls_Col_Fill_NA' : ls_Col_Fill_NA ,'ls_Col_Cat': ls_Col_Cat}\n",
    "\n",
    "# Add timestamp to the file\n",
    "sOutputFileName = sOutputFileBase + '_' + dt.datetime.now().strftime(\"%m-%d-%y-%H%M%S\") + '.pck'\n",
    "\n",
    "# Save this as pickle so it can be arhived\n",
    "if bSaveOutputPickle:\n",
    "    pickle.dump(dict_Output,open(sOutputFileName,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# End of Script\n",
    "# -------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
